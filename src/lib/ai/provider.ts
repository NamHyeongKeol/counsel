import OpenAI from "openai";
import Anthropic from "@anthropic-ai/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";

import {
    AIProvider,
    AI_MODELS,
    AIModelId,
    Message,
    ChatOptions,
    ChatResult,
    StreamCallbacks
} from "./constants";

// OpenAI Client (also used for Grok/xAI and Deepseek)
function getOpenAIClient(provider: AIProvider): OpenAI {
    switch (provider) {
        case "xai":
            return new OpenAI({
                apiKey: process.env.XAI_API_KEY,
                baseURL: "https://api.x.ai/v1",
            });
        case "deepseek":
            return new OpenAI({
                apiKey: process.env.DEEPSEEK_API_KEY,
                baseURL: "https://api.deepseek.com",
            });
        default:
            return new OpenAI({
                apiKey: process.env.OPENAI_API_KEY,
            });
    }
}

function getTechnicalModelName(modelId: AIModelId): string {
    switch (modelId) {
        case "gemini-3-flash-preview": return "gemini-3-flash-preview";
        case "gemini-3-pro-preview": return "gemini-3-pro-preview";
        case "claude-opus-4-5-20251101": return "claude-opus-4-5-20251101";
        case "gpt-5.2": return "gpt-5.2";
        default: return "gemini-3-flash-preview";
    }
}

function getModelName(provider: AIProvider): string {
    switch (provider) {
        case "openai":
            return "gpt-4o";
        case "xai":
            return "grok-2-latest";
        case "deepseek":
            return "deepseek-chat";
        default:
            return "gpt-4o";
    }
}

async function chatWithOpenAI(
    messages: Message[],
    provider: AIProvider,
    systemPrompt: string
): Promise<ChatResult> {
    const client = getOpenAIClient(provider);
    const model = getModelName(provider);

    const response = await client.chat.completions.create({
        model,
        messages: [
            { role: "system", content: systemPrompt },
            ...messages.map((m) => ({ role: m.role, content: m.content })),
        ],
        max_tokens: 1024,
        temperature: 0.8,
    });

    return {
        content: response.choices[0]?.message?.content || "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ë¬¸ì œê°€ ìˆì—ˆì–´ìš”.",
        model,
        inputTokens: response.usage?.prompt_tokens ?? null,
        outputTokens: response.usage?.completion_tokens ?? null,
    };
}

async function chatWithAnthropic(messages: Message[], systemPrompt: string, modelId: AIModelId = "claude-opus-4-5-20251101"): Promise<ChatResult> {
    const client = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
    });

    const technicalModel = getTechnicalModelName(modelId);

    const response = await client.messages.create({
        model: technicalModel,
        max_tokens: 1024,
        system: systemPrompt,
        messages: messages.map((m) => ({
            role: m.role,
            content: m.content,
        })),
    });

    const textBlock = response.content.find((block) => block.type === "text");
    return {
        content: textBlock?.type === "text"
            ? textBlock.text
            : "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ë¬¸ì œê°€ ìˆì—ˆì–´ìš”.",
        model: technicalModel,
        inputTokens: response.usage?.input_tokens ?? null,
        outputTokens: response.usage?.output_tokens ?? null,
    };
}

async function chatWithGoogle(messages: Message[], systemPrompt: string, modelId: AIModelId = "gemini-3-flash-preview"): Promise<ChatResult> {
    const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY || "");
    const technicalModel = getTechnicalModelName(modelId);
    const model = genAI.getGenerativeModel({
        model: technicalModel,
        systemInstruction: systemPrompt,
    });

    // Google AIëŠ” ì²« ë©”ì‹œì§€ê°€ userì—¬ì•¼ í•¨ - modelì´ë©´ ë”ë¯¸ user ë©”ì‹œì§€ ì¶”ê°€
    let adjustedMessages = [...messages];
    if (adjustedMessages.length > 0 && adjustedMessages[0].role === "assistant") {
        adjustedMessages = [
            { role: "user" as const, content: "ì•ˆë…•í•˜ì„¸ìš”" },
            ...adjustedMessages
        ];
    }

    const chat = model.startChat({
        history: adjustedMessages.slice(0, -1).map((m) => ({
            role: m.role === "user" ? "user" : "model",
            parts: [{ text: m.content }],
        })),
    });

    const lastMessage = adjustedMessages[adjustedMessages.length - 1];
    const result = await chat.sendMessage(lastMessage.content);
    const usageMetadata = result.response.usageMetadata;

    return {
        content: result.response.text(),
        model: technicalModel,
        inputTokens: usageMetadata?.promptTokenCount ?? null,
        outputTokens: usageMetadata?.candidatesTokenCount ?? null,
    };
}

export async function chat(options: ChatOptions): Promise<ChatResult> {
    const modelId = options.modelId || "gemini-3-flash-preview";
    const provider = AI_MODELS[modelId]?.provider || options.provider || "google";

    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ìŒ (ìºë¦­í„°ì˜ systemPrompt)
    const systemPrompt = options.systemPrompt || "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.";

    const startTime = Date.now();

    try {
        let result: ChatResult;
        switch (provider) {
            case "anthropic":
                result = await chatWithAnthropic(options.messages, systemPrompt, modelId);
                break;
            case "google":
                result = await chatWithGoogle(options.messages, systemPrompt, modelId);
                break;
            case "openai":
            case "xai":
            case "deepseek":
                result = await chatWithOpenAI(options.messages, provider, systemPrompt);
                break;
            default:
                result = await chatWithOpenAI(options.messages, "openai", systemPrompt);
        }

        // ğŸ” ì„œë²„ ë¡œê·¸: AI ì‘ë‹µ ì •ë³´
        const duration = Date.now() - startTime;
        console.log("\n" + "-".repeat(60));
        console.log("âœ… [AI Response]");
        console.log("-".repeat(60));
        console.log(`â±ï¸  Duration: ${duration}ms`);
        console.log(`ğŸ”¢ Tokens: input=${result.inputTokens}, output=${result.outputTokens}`);
        console.log("-".repeat(60) + "\n");

        return result;
    } catch (error) {
        console.error("\nâŒ [AI Error]", error);
        throw new Error("AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    }
}

export async function streamChat(options: ChatOptions, callbacks: StreamCallbacks) {
    const modelId = options.modelId || "gemini-3-flash-preview";
    const provider = AI_MODELS[modelId]?.provider || "google";

    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ìŒ
    const systemPrompt = options.systemPrompt || "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.";

    try {
        if (provider === "google") {
            await streamGoogle(options.messages, systemPrompt, modelId, callbacks);
        } else if (provider === "anthropic") {
            await streamAnthropic(options.messages, systemPrompt, modelId, callbacks);
        } else if (provider === "openai") {
            await streamOpenAI(options.messages, systemPrompt, modelId, callbacks);
        } else {
            // ë‹¤ë¥¸ ì œê³µìëŠ” í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ë¯¸êµ¬í˜„ (í•„ìš”ì‹œ ì¶”ê°€)
            const result = await chat(options);
            callbacks.onChunk(result.content);
            callbacks.onDone(result.content, { inputTokens: result.inputTokens, outputTokens: result.outputTokens });
        }
    } catch (error) {
        callbacks.onError(error);
    }
}

async function streamGoogle(messages: Message[], systemPrompt: string, modelId: AIModelId, callbacks: StreamCallbacks) {
    const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY || "");
    const technicalModel = getTechnicalModelName(modelId);
    const model = genAI.getGenerativeModel({
        model: technicalModel,
        systemInstruction: systemPrompt,
    });

    let adjustedMessages = [...messages];
    if (adjustedMessages.length > 0 && adjustedMessages[0].role === "assistant") {
        adjustedMessages = [{ role: "user" as const, content: "ì•ˆë…•í•˜ì„¸ìš”" }, ...adjustedMessages];
    }

    const chat = model.startChat({
        history: adjustedMessages.slice(0, -1).map((m) => ({
            role: m.role === "user" ? "user" : "model",
            parts: [{ text: m.content }],
        })),
    });

    const lastMessage = adjustedMessages[adjustedMessages.length - 1];
    const result = await chat.sendMessageStream(lastMessage.content);

    let fullText = "";
    for await (const chunk of result.stream) {
        const text = chunk.text();
        fullText += text;
        callbacks.onChunk(text);
    }

    const response = await result.response;
    const usage = response.usageMetadata;

    callbacks.onDone(fullText, {
        inputTokens: usage?.promptTokenCount ?? null,
        outputTokens: usage?.candidatesTokenCount ?? null,
    });
}

async function streamAnthropic(messages: Message[], systemPrompt: string, modelId: AIModelId, callbacks: StreamCallbacks) {
    const client = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
    });

    const technicalModel = getTechnicalModelName(modelId);

    const stream = await client.messages.create({
        model: technicalModel,
        max_tokens: 1024,
        system: systemPrompt,
        messages: messages.map((m) => ({
            role: m.role,
            content: m.content,
        })),
        stream: true,
    });

    let fullText = "";
    let inputTokens = null;
    let outputTokens = null;

    for await (const event of stream) {
        if (event.type === "message_start") {
            inputTokens = event.message.usage.input_tokens;
        } else if (event.type === "content_block_delta") {
            if (event.delta.type === "text_delta") {
                const text = event.delta.text;
                fullText += text;
                callbacks.onChunk(text);
            }
        } else if (event.type === "message_delta") {
            outputTokens = event.usage.output_tokens;
        }
    }

    callbacks.onDone(fullText, { inputTokens, outputTokens });
}

async function streamOpenAI(messages: Message[], systemPrompt: string, modelId: AIModelId, callbacks: StreamCallbacks) {
    const client = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
    });

    const technicalModel = getTechnicalModelName(modelId);

    const stream = await client.chat.completions.create({
        model: technicalModel,
        max_completion_tokens: 1024,
        messages: [
            { role: "system", content: systemPrompt },
            ...messages.map((m) => ({ role: m.role as "user" | "assistant", content: m.content })),
        ],
        stream: true,
        stream_options: { include_usage: true },
    });

    let fullText = "";
    let inputTokens: number | null = null;
    let outputTokens: number | null = null;

    for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta;
        if (delta?.content) {
            fullText += delta.content;
            callbacks.onChunk(delta.content);
        }
        // ì‚¬ìš©ëŸ‰ ì •ë³´ (ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ ì œê³µë  ìˆ˜ ìˆìŒ)
        if (chunk.usage) {
            inputTokens = chunk.usage.prompt_tokens;
            outputTokens = chunk.usage.completion_tokens;
        }
    }

    callbacks.onDone(fullText, { inputTokens, outputTokens });
}
